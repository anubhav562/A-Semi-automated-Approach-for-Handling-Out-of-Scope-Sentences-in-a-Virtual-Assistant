{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize,wordpunct_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer =  WordNetLemmatizer()\n",
    "\n",
    "# Loading Stopwords\n",
    "with open(\"StopWords.txt\") as f:\n",
    "    stop_words = f.read()\n",
    "\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "stop_words_dict = {}\n",
    "for stop_word in stop_words:\n",
    "    stop_words_dict[stop_word] = 1\n",
    "\n",
    "# Function to remove stopwords,numeric and alpha numeric characters  \n",
    "def _remove_stop_words_keep_alphabetic_tokens(sentence):\n",
    "    tokens = sentence.split()\n",
    "    new_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        \n",
    "        if token in stop_words_dict:\n",
    "            continue\n",
    "        elif not token.isalpha():\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            \n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "# # Function to calculate sentence embeddings\n",
    "def calculate_sentence_embeddings(tokens):\n",
    "    Vector_mean=np.mean(glove_embedding.vectors,axis=0)\n",
    "    word_embedding = np.zeros((len(tokens),glove_embedding.vector_size))\n",
    "    for index,word in enumerate(tokens):\n",
    "        \n",
    "        if word in glove_embedding.vocab.keys():\n",
    "            embedding = glove_embedding.get_vector(word)\n",
    "            word_embedding[index,:] = embedding\n",
    "        else:\n",
    "            word_embedding[index,:] = Vector_mean\n",
    "    \n",
    "    sentence_embeddings = (np.sum(word_embedding,axis=0))/len(tokens)\n",
    "    \n",
    "    return sentence_embeddings\n",
    "\n",
    "# Funtion to remove stop words\n",
    "def _remove_stop_words_from_sentence(sentence):\n",
    "    tokens = sentence.split()\n",
    "    new_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token in stop_words_dict:\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            \n",
    "    return \" \".join(new_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       i need $20000 transferred from my savings to m...\n",
       "1       complete a transaction from savings to checkin...\n",
       "2       transfer $20000 from my savings account to che...\n",
       "3         take $20000 from savings and put it in checking\n",
       "4       put $20000 into my checking account from my sa...\n",
       "                              ...                        \n",
       "2245                              give weather update now\n",
       "2246                             want to know the weather\n",
       "2247                        tell me the weather for today\n",
       "2248                     what is the current weather like\n",
       "2249                              las vegas weather today\n",
       "Name: utterance, Length: 2250, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the required dataset\n",
    "utterances_data = pd.read_csv(\"utterance_data.csv\")\n",
    "utterances_data[\"utterance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading word2vec pretrained model for english language\n",
    "glove_embedding = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering without removing stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing sentences \n",
    "tokenized_data = utterances_data[\"utterance\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fectching emebeddings for the utterances data\n",
    "\n",
    "data_embeddings = np.zeros((tokenized_data.shape[0],glove_embedding.vector_size))\n",
    "\n",
    "for index,sentence in enumerate(tokenized_data):\n",
    "    data_embeddings[index,:]=calculate_sentence_embeddings(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 5 clusters, got a Silhoutte Score of 0.16434900831205654\n"
     ]
    }
   ],
   "source": [
    "num_clusters =5\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 10 clusters, got a Silhoutte Score of 0.1627663607728707\n"
     ]
    }
   ],
   "source": [
    "num_clusters =10\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 15 clusters, got a Silhoutte Score of 0.20488595789482783\n"
     ]
    }
   ],
   "source": [
    "num_clusters =15\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwordsand tokenizing sentences \n",
    "no_stopwords_tokenized_data = utterances_data[\"utterance\"].apply(_remove_stop_words_from_sentence).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fectching emebeddings for the utterances data\n",
    "\n",
    "no_stopwords_data_embeddings = np.zeros((no_stopwords_tokenized_data.shape[0],glove_embedding.vector_size))\n",
    "\n",
    "for index,sentence in enumerate(no_stopwords_tokenized_data):\n",
    "    no_stopwords_data_embeddings[index,:]=calculate_sentence_embeddings(sentence)\n",
    "\n",
    "no_stopwords_data_embeddings = pd.DataFrame(no_stopwords_data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords_data_embeddings.fillna(value=no_stopwords_data_embeddings.mean(axis=0),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 5 clusters, got a Silhoutte Score of 0.19795586113360233\n"
     ]
    }
   ],
   "source": [
    "num_clusters =5\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(no_stopwords_data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(no_stopwords_data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 10 clusters, got a Silhoutte Score of 0.2769502434359487\n"
     ]
    }
   ],
   "source": [
    "num_clusters =10\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(no_stopwords_data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(no_stopwords_data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 15 clusters, got a Silhoutte Score of 0.2769482453398693\n"
     ]
    }
   ],
   "source": [
    "num_clusters =15\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(no_stopwords_data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(no_stopwords_data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Removing stopwords,nunmeric and alpha numeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords,,nunmeric and alpha numeric  and tokenizing sentences \n",
    "preprocessed_tokenized_data = utterances_data[\"utterance\"].apply(_remove_stop_words_keep_alphabetic_tokens).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-4272b41c3ccb>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sentence_embeddings = (np.sum(word_embedding,axis=0))/len(tokens)\n"
     ]
    }
   ],
   "source": [
    "# fectching emebeddings for the utterances data\n",
    "\n",
    "preprocessed_data_embeddings = np.zeros((preprocessed_tokenized_data.shape[0],glove_embedding.vector_size))\n",
    "\n",
    "for index,sentence in enumerate(preprocessed_tokenized_data):\n",
    "    preprocessed_data_embeddings[index,:]=calculate_sentence_embeddings(sentence)\n",
    "\n",
    "preprocessed_data_embeddings = pd.DataFrame(preprocessed_data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_embeddings.fillna(value=preprocessed_data_embeddings.mean(axis=0),inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 5 clusters, got a Silhoutte Score of 0.21104735684723155\n"
     ]
    }
   ],
   "source": [
    "num_clusters =5\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(preprocessed_data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(preprocessed_data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 10 clusters, got a Silhoutte Score of 0.29494133011001644\n"
     ]
    }
   ],
   "source": [
    "num_clusters =10\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(preprocessed_data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(preprocessed_data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 15 clusters, got a Silhoutte Score of 0.3561676161352428\n"
     ]
    }
   ],
   "source": [
    "num_clusters =15\n",
    "k_means_obj = KMeans(n_clusters=num_clusters)\n",
    "k_means_obj.fit(preprocessed_data_embeddings)\n",
    "labels = k_means_obj.labels_\n",
    "\n",
    "## Calculating silhoutte score\n",
    "## The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.\n",
    "\n",
    "sil_score = silhouette_score(preprocessed_data_embeddings,labels, metric=\"cosine\")\n",
    "\n",
    "print(f\"Fitted {num_clusters} clusters, got a Silhoutte Score of {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
